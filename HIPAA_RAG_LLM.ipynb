{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0974e7fd-d070-4eec-92c0-a74d9c382117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_docling import DoclingLoader\n",
    "\n",
    "from langchain_together import ChatTogether\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langgraph.graph import MessagesState, StateGraph\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f1b4e68-27a3-4118-ac7e-5d47cd104cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter API key for langchain ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter API key for langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ac147b2-9509-453a-9aa7-d673f3a5b589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter API key for Together AI:  ········\n"
     ]
    }
   ],
   "source": [
    "if not os.environ.get(\"TOGETHER_API_KEY\"):\n",
    "  os.environ[\"TOGETHER_API_KEY\"] = getpass.getpass(\"Enter API key for Together AI: \")\n",
    "\n",
    "llm = ChatTogether(\n",
    "    model=\"meta-llama/Llama-3-70b-chat-hf\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=os.environ[\"TOGETHER_API_KEY\"],\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e55e023-744a-4a19-b56f-8c69064d4278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\I'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\I'\n",
      "C:\\Users\\moyee\\AppData\\Local\\Temp\\ipykernel_27000\\2091304677.py:2: SyntaxWarning: invalid escape sequence '\\I'\n",
      "  FILE_PATH = root_dir + \"\\Input.docx\"\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (551 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "root_dir = os.getcwd()\n",
    "FILE_PATH = root_dir + \"\\Input.docx\"\n",
    "\n",
    "loader = DoclingLoader(file_path=FILE_PATH)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9875de42-8905-42f1-9632-dadb2fb72bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index chunks\n",
    "_ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "\n",
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "def query_or_respond(state: MessagesState):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "\n",
    "# Step 3: Generate a response using the retrieved content.\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77025a88-e831-4245-8cc0-e252fe676e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "# Specify an ID for the thread\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed1c7304-98f7-45c1-8550-2a0720ebca57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi when is a CPA a business associate?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Under the Health Insurance Portability and Accountability Act (HIPAA), a Certified Public Accountant (CPA) can be considered a Business Associate (BA) in certain circumstances.\n",
      "\n",
      "A Business Associate is a person or organization that performs certain functions or activities that involve the use or disclosure of protected health information (PHI) on behalf of a Covered Entity (CE). Covered Entities include healthcare providers, health plans, and healthcare clearinghouses.\n",
      "\n",
      "A CPA may be considered a Business Associate if they provide services to a Covered Entity that involve the use or disclosure of PHI. Here are some examples:\n",
      "\n",
      "1. **Accounting and auditing services**: If a CPA firm provides accounting or auditing services to a healthcare provider or health plan, and in the course of those services, they have access to PHI, they may be considered a Business Associate.\n",
      "2. **Financial analysis and consulting**: If a CPA firm provides financial analysis or consulting services to a healthcare organization, and those services involve the use or disclosure of PHI, they may be considered a Business Associate.\n",
      "3. **Tax preparation and planning**: If a CPA firm prepares tax returns or provides tax planning services to a healthcare provider or health plan, and those services involve the use or disclosure of PHI, they may be considered a Business Associate.\n",
      "4. **IT and data analytics services**: If a CPA firm provides IT or data analytics services to a healthcare organization, and those services involve the use or disclosure of PHI, they may be considered a Business Associate.\n",
      "\n",
      "To determine whether a CPA is a Business Associate, consider the following factors:\n",
      "\n",
      "* Does the CPA have access to PHI in the course of providing services to the Covered Entity?\n",
      "* Does the CPA use or disclose PHI in the course of providing services to the Covered Entity?\n",
      "* Is the CPA performing a function or activity on behalf of the Covered Entity that involves the use or disclosure of PHI?\n",
      "\n",
      "If the answer to any of these questions is yes, the CPA may be considered a Business Associate and would be subject to the requirements of HIPAA, including the need to enter into a Business Associate Agreement (BAA) with the Covered Entity.\n"
     ]
    }
   ],
   "source": [
    "input_message = \"Hi when is a CPA a business associate?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87338d4e-d8e2-4424-b395-6fcfb23b2dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_graph_response(input_message):\n",
    "    result = \"\"\n",
    "    for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "    ):\n",
    "        result += step[\"messages\"][-1].pretty_repr()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97f368b9-5b74-4616-812d-ee8a0f061c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "CORS(app)  # Allows requests from any origin\n",
    "\n",
    "@app.route('/get-ai-message', methods=['POST'])\n",
    "def get_ai_message():\n",
    "    data = request.json\n",
    "    if not data:\n",
    "        return jsonify({\"error\": \"Invalid request\"}), 400\n",
    "\n",
    "    user_query = data.get('userQuery')\n",
    "    response = stream_graph_response(user_query)\n",
    "    \n",
    "    return jsonify({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a55e1a-631c-47f9-8b9c-1ad60185adcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
